\section*{Introduction}

Les algorithmes de recommandations sont au coeur de la stratégie des entreprises de e-commerce afin de proposer à leurs
clients des produits qui pourraient correspondre à leurs besoins. Ces algorithmes de recommandations, déjà éprouvés, se
basent souvent sur l'établissement d'un profil utilisateur en fonction de son comportement sur le site : clics, pages
visitées et produits achetés. Ce comportement est ensuite comparé à la liste des produits et aux comportements des
autres clients à travers différentes méthodes, comme le "collaborative filtering" ou des modèles de clusters
\cite{amazon2003linden}. Cependant, ces méthodes souffrent de nombreux défauts, comme un coût de calcul prohibitif
empêchant un passage à grande échelle, ainsi qu'une précision faible \cite{deep2017shankar}.

Enfin, un autre problème majeur de ses techniques est qu'elles reposent souvent sur des métadonnées ou des données
textuelles. Or, dans le domaine de la mode, les utilisateurs souhaitent pouvoir chercher des produits et obtenir des
recommandations basées sur des critères visuels. Il est donc crucial pour les sites de e-commerce, qui possèdent de
grandes bases de données d'images mais non étiquetées, de pouvoir proposer des systèmes de recommandations basés sur
ces critères visuels afin d’atteindre un “shazam des produits”. Ce projet a pour objectif de faire les premiers pas
dans cette direction. 

\subsection*{État de l'art}

Grâce aux récents développements des techniques de Machine Learning et de Deep Learning, les systèmes de classification
et de détection d’images sont maintenant à des niveaux de performance égalant les humains sur certaines tâches. Ils
permettent d’extraire de manière automatique les informations essentielles caractérisant les images.  Le coeur des
techniques de matching repose sur l’apprentissage d’une représentation latente des images. C’est ainsi dans cet espace
que l’étude de similarité est alors possible.

Pour approcher ce problème, nous avons tout d'abord lu la littérature récente sur le sujet. Ainsi, nous avons lu des
papiers de eBay, Pinterest. La tendance globale que nous avons observé est une utilisation de features extraites d'un
réseau de neurones : les images sont projetées sont dans un espace vectoriel de dimension plus petite que la taille
originale qui permet de séparer mieux les différentes classes.

Toutefois, nous nous sommes retrouvés confrontés à un problème : ces entités possèdent un nombre gigantesque de données
auxquels une personne lambda n'a pas accès. 

Par ailleurs, cette quantité de données possèdent parfois une structure particulière qui rend possible l'utilisation de
méthodes spécialisées : Pinterest, par son statut de réseau social, possèdent des données organisées en graphes, et
utilise donc des méthodes spécialisées. Ainsi, dans leur article \cite{ying2018graph}, Pinterest utilise des
\textit{Graph Convolutional Neural Networks} (GCN), ce qui est possible uniquement grâce à la structure sous-jacente de
graphe d'un réseau social.

De même, eBay utilise dans \cite{yang2017visual} des métadonnées associées aux produits de sa plateforme. Dans cet
article, eBay utilise un système de couches pour un retrieval précis et rapide à grande échelle : pour une image requête
particulière, leur système utilise différents critères pour épurer petit à petit les résultats potentiels. Ils utilisent
d'abord des features peu coûteuses à calculer et dont la comparaison est simple.

Nous nous sommes concentrés sur l'aspect "Image Retrieval" en prenant compte des features extraites de réseaux de
neurones profonds sans métadonnées. Pour cette extraction de features, nous allons comparer différents modèles que nous
allons présenter après.
