{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import pickle as pk\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "from keras.layers import UpSampling2D, Input\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model\n",
    "inp = Input(shape=(32, 32, 3), name='image_input')\n",
    "x = UpSampling2D(size =(7,7))(inp)\n",
    "resnet = ResNet50(weights='imagenet', include_top=True)\n",
    "\n",
    "resnet.layers.pop()\n",
    "resnet.outputs = [resnet.layers[-1].output]\n",
    "resnet.layers[-1].outbound_nodes = []\n",
    "resnet.summary()\n",
    "output = resnet(x)\n",
    "\n",
    "model = Model(inputs=inp, outputs=output)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load a batch into memory\n",
    "def load_batch(data_dir, batch_id):\n",
    "    with open(os.path.join(data_dir, 'data_batch_%i' % batch_id), mode='rb') as file:\n",
    "        batch = pk.load(file, encoding='latin1')\n",
    "    feats = batch['data'].reshape((len(batch['data']), 3, 32, 32)).transpose(0, 2, 3, 1)\n",
    "    lbls = batch['labels']\n",
    "    return feats, lbls\n",
    "\n",
    "# Load the first batch\n",
    "data_dir = '../data/'\n",
    "feats, labels = load_batch(data_dir + 'cifar10', 1)\n",
    "label_names = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize images\n",
    "sample_id = 19\n",
    "sample_img = feats[sample_id]\n",
    "sample_lbl = labels[sample_id]\n",
    "print('Label Id: {} - Class: {}'.format(sample_lbl, label_names[sample_lbl]))\n",
    "plt.imshow(sample_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_img = preprocess_input(sample_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 3000\n",
    "sample_feats = feats[:sample_size,:,:,:]\n",
    "sample_labels = labels[:sample_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import json\n",
    "comp_embeddings = False\n",
    "dump_embeddings = False\n",
    "if comp_embeddings:\n",
    "    start = time.time()\n",
    "    embeddings = model.predict(feats[:sample_size,:,:,:])\n",
    "    print(time.time()-start)\n",
    "    \n",
    "if dump_embeddings:\n",
    "    with open(f'embeddings/embeddings_{sample_size}.json', 'w') as outfile:\n",
    "        json.dump(embeddings.tolist(), outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_from_json(file:str):\n",
    "    return json.loads(open(f'embeddings/{file}').read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import scale\n",
    "def split_set(embeddings, labels, sample_size):\n",
    "    n_embeddings= scale(embeddings)\n",
    "    indices = np.random.permutation(sample_size)\n",
    "    training_idx, dev_idx,test_idx = indices[:int(0.6*sample_size)], indices[int(0.6*sample_size):int(0.8*sample_size)],indices[int(0.8*sample_size):]\n",
    "    feats_train,feats_dev, feats_test = n_embeddings[training_idx,:],n_embeddings[dev_idx,:], n_embeddings[test_idx,:]\n",
    "    labels_train,labels_dev, labels_test = [np.array(labels[i]) for i in training_idx],[np.array(labels[i]) for i in dev_idx], [np.array(labels[i]) for i in test_idx]\n",
    "    return [feats_train,feats_dev, feats_test],[labels_train,labels_dev, labels_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "def train_knn_and_test(train,dev,labels_train,labels_dev):\n",
    "    knn = KNeighborsClassifier(n_neighbors=10)\n",
    "    knn.fit(train,labels_train)\n",
    "    return knn,knn.score(train, labels_train), knn.score(dev, labels_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "def train_pca(train_set, percentage=90):\n",
    "    for i in range(50,min(sample_size,4096),50):\n",
    "        pca = PCA(n_components=i)\n",
    "        pca.fit(train_set)\n",
    "        if np.sum(100*pca.explained_variance_ratio_) > percentage:\n",
    "            return i,pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "\n",
    "for file in listdir('./embeddings'):\n",
    "    if '.json' in file:\n",
    "        df = pd.DataFrame(columns=['PCA','Train accuracy', 'Dev accuracy'])\n",
    "        sample_size = int(file[file.index('_')+1:file.index('.')])\n",
    "        embeddings = load_from_json(file)\n",
    "        \n",
    "        # Spit the data\n",
    "        [feats_train,feats_dev, feats_test],[labels_train,labels_dev, labels_test] = split_set(\n",
    "            embeddings, labels[:sample_size], sample_size)\n",
    "\n",
    "        # Train knn\n",
    "        knn, knn_train_score, knn_test_score = train_knn_and_test(feats_train,feats_dev,labels_train,labels_dev)\n",
    "        \n",
    "        df.loc[0] = [0,knn_train_score,knn_test_score ]\n",
    "        \n",
    "        # Fit pca\n",
    "        i,pca = train_pca(feats_train)\n",
    "        print(f'Trained pca with {i} components')\n",
    "        \n",
    "        # Train knn\n",
    "        knn, knn_train_score, knn_test_score = train_knn_and_test(pca.transform(feats_train),pca.transform(feats_dev),labels_train,labels_dev)\n",
    "        df.loc[1] = [1,knn_train_score,knn_test_score]\n",
    "        print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(knn.predict(pca.transform(feats_dev)), labels_dev)\n",
    "#['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time to compute 100 embeddings\n",
    "nb_samples = 100\n",
    "start = time.time()\n",
    "embeddings2 = model.predict(feats[:nb_samples,:,:,:])\n",
    "print(time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
